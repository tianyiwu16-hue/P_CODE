import os
import re
import csv
import time
import zipfile
import requests
from bs4 import BeautifulSoup

student_id = "2024113419"
name = "è®¸å¿—å›½"
classname = "æ•°æ®2402"

folder_name = f"{student_id}_{name}_{classname}"
data_folder = os.path.join(folder_name, "data")
csv_file = os.path.join(folder_name, "newslist.csv")
zip_name = f"{folder_name}.zip"

os.makedirs(data_folder, exist_ok=True)

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

base_url = "https://finance.eastmoney.com/yaowen.html"
print("æ­£åœ¨æŠ“å–è´¢ç»æ–°é—»ç½‘é¡µ...")

resp = requests.get(base_url, headers=headers, timeout=(5,10))
resp.encoding = "utf-8"
soup = BeautifulSoup(resp.text, "html.parser")

links = []
for a in soup.find_all("a", href=True):
    href = a["href"]
    title = a.get_text(strip=True)
    # è¿‡æ»¤ä»…ä¿ç•™è´¢ç»æ–°é—»é¡µé¢
    if re.match(r"^https?://finance\.eastmoney\.com/a/\d{8,}", href):
        links.append((title, href))

print(f"æ‰¾åˆ° {len(links)} æ¡æ½œåœ¨æ–°é—»é“¾æ¥")

news_data = []
titles_seen = set()

for i, (title, link) in enumerate(links[:30]):  # é™åˆ¶30æ¡
    if not title or title in titles_seen:
        continue
    titles_seen.add(title)
    try:
        detail = requests.get(link, headers=headers, timeout=(5,10))
        detail.encoding = "utf-8"
        text = BeautifulSoup(detail.text, "html.parser").get_text("\n", strip=True)
    except Exception as e:
        text = f"ã€æ­£æ–‡è·å–å¤±è´¥ï¼š{e}ã€‘"

    safe_title = "".join(c for c in title if c not in "\\/:*?\"<>|")[:50]
    txt_path = os.path.join(data_folder, f"{safe_title}.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(text)

    news_data.append([title, link])
    print(f"âœ… æŠ“å–æˆåŠŸï¼š{title}")
    time.sleep(0.5)

# ä¿å­˜CSV
with open(csv_file, "w", encoding="utf-8-sig", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["æ ‡é¢˜", "é“¾æ¥"])
    writer.writerows(news_data)

# æ‰“åŒ…ZIP
with zipfile.ZipFile(zip_name, "w", zipfile.ZIP_DEFLATED) as zipf:
    for root, dirs, files in os.walk(folder_name):
        for file in files:
            path = os.path.join(root, file)
            arc = os.path.relpath(path, start=os.path.dirname(folder_name))
            zipf.write(path, arc)

print(f"ğŸ‰ å…±æŠ“å– {len(news_data)} æ¡æ–°é—»ï¼Œæ•°æ®å·²æ‰“åŒ…ï¼š{zip_name}")

print("å‹ç¼©æ–‡ä»¶å®Œæ•´è·¯å¾„ï¼š", os.path.abspath(zip_name))












import requests
import json

# å®šä¹‰ API URL
url = "https://guba.eastmoney.com/api/getData?path=data/api/Data/GetIndexData"

# å®šä¹‰è¯·æ±‚å¤´ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

# å‘é€ GET è¯·æ±‚
response = requests.get(url, headers=headers)

# æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
if response.status_code == 200:
    data = response.json()  # è§£æè¿”å›çš„ JSON æ•°æ®
    
    # æ‰“å°æ•°æ®ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¿™éƒ¨åˆ†ä»£ç æ¥ä¿å­˜æˆ–å¤„ç†æ•°æ®ï¼‰
    print(json.dumps(data, indent=4, ensure_ascii=False))
else:
    print("è·å–æ•°æ®å¤±è´¥ï¼ŒçŠ¶æ€ç :", response.status_code)














import requests
import json
import time

# === é…ç½®å‚æ•° ===
stock_code = "600519"  # è‚¡ç¥¨ä»£ç ï¼ˆå¿…é¡»æ˜¯çº¯æ•°å­—ï¼‰
page = 1
page_size = 20

url = "https://guba.eastmoney.com/api/getData"

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': f'https://guba.eastmoney.com/list,{stock_code}.html',
    'Content-Type': 'application/json;charset=UTF-8',
    'Accept': 'application/json'
}

payload = {
    "path": "data/api/Data/GetIndexData",
    "param": {
        "code": stock_code,
        "sort": "time",        # "time" æœ€æ–°å‘å¸–ï¼Œ"reply" å›å¤æœ€å¤š
        "page": page,
        "pageSize": page_size
    }
}

try:
    response = requests.post(url, headers=headers, json=payload, timeout=10)
    print("Status Code:", response.status_code)
    
    if response.status_code == 200:
        data = response.json()
        # æ‰“å°åŸå§‹å“åº”ï¼ˆè°ƒè¯•ç”¨ï¼‰
        # print("Raw Response:", json.dumps(data, indent=2, ensure_ascii=False))
        
        if data.get("re") and data.get("result"):
            posts = data["result"]
            print(f"\nâœ… æˆåŠŸè·å– {len(posts)} æ¡å¸–å­\n")
            
            for i, post in enumerate(posts, 1):
                title = post.get("title", "æ— æ ‡é¢˜")
                author = post.get("author", "åŒ¿å")
                create_time = post.get("createTime", "")
                reply_count = post.get("replyCount", 0)
                is_top = post.get("isTop", False)
                
                if is_top:
                    continue  # è·³è¿‡ç½®é¡¶
                
                print(f"{i}. [{create_time}] {title} â€”â€” @{author} ({reply_count} å›å¤)")
        else:
            print("âŒ API è¿”å› re=false æˆ– result ä¸ºç©º")
            print("Response:", data)
    else:
        print("âŒ HTTP è¯·æ±‚å¤±è´¥:", response.text)

except Exception as e:
    print("âš ï¸ å¼‚å¸¸:", e)















































